{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75122bbf",
   "metadata": {},
   "source": [
    "# NLP A4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74790965",
   "metadata": {},
   "source": [
    "## Set up the dataset\n",
    "\n",
    "We implement a BERT-style encoder with masked language modeling (MLM) \n",
    "and next sentence prediction (NSP), and pretrain it on a subset of a public corpus \n",
    "(BookCorpus / English Wikipedia).\n",
    "\n",
    "**References**\n",
    "\n",
    "[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018).  \n",
    "[2] BookCorpus dataset: https://huggingface.co/datasets/bookcorpus  \n",
    "[3] English Wikipedia dataset: https://huggingface.co/datasets/legacy-datasets/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a3bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from random import random, shuffle, randint\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2bc984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda is ready\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27949b1a",
   "metadata": {},
   "source": [
    "### Code: โหลด corpus (เอาตัวอย่าง BookCorpus subset 100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ae6840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 74004228\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load bookcorpus as plain text\n",
    "raw_dataset = load_dataset(\"bookcorpus\", \"plain_text\", trust_remote_code=True)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d7c093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72044730"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract text list from split train\n",
    "texts = raw_dataset[\"train\"][\"text\"]\n",
    "\n",
    "# Filter for short sentense/null sentenses and fliter lower case\n",
    "clean_texts = []\n",
    "for t in texts:\n",
    "    if t is None:\n",
    "        continue\n",
    "    t = t.strip()\n",
    "    if len(t) < 10:\n",
    "        continue\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"[^a-z0-9 ,.!?'-]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    if t:\n",
    "        clean_texts.append(t)\n",
    "\n",
    "len(clean_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69aa2674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ramdomly pick 100k subset (Lower if GPU is not strong enough)\n",
    "np.random.seed(42)\n",
    "max_samples = min(100_000, len(clean_texts))\n",
    "indices = np.random.choice(len(clean_texts), size=max_samples, replace=False)\n",
    "corpus = [clean_texts[i] for i in indices]\n",
    "len(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f22ec3",
   "metadata": {},
   "source": [
    "### Code: สร้าง vocab แบบ word-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e40ba17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44623,\n",
       " [('[PAD]', 0),\n",
       "  ('[CLS]', 1),\n",
       "  ('[SEP]', 2),\n",
       "  ('[MASK]', 3),\n",
       "  ('[UNK]', 4),\n",
       "  ('knuckle', 5),\n",
       "  ('whimper', 6),\n",
       "  ('alrighty', 7),\n",
       "  ('exhalations', 8),\n",
       "  ('married', 9)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocab from corpus\n",
    "special_tokens = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    "word2id = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "start_idx = len(special_tokens)\n",
    "\n",
    "all_words = set(\" \".join(corpus).split())\n",
    "for i, w in enumerate(all_words):\n",
    "    word2id[w] = i + start_idx\n",
    "\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "vocab_size, list(word2id.items())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "259739c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word2id as json format\n",
    "import json\n",
    "os.makedirs(\"../dataset\", exist_ok=True)\n",
    "with open(\"../dataset/word2id.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8803dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each sentenses to list of ids (Not included CLS/SEP)\n",
    "tokenized_sentences = []\n",
    "for sent in corpus:\n",
    "    ids = [word2id[w] for w in sent.split() if w in word2id]\n",
    "    if len(ids) > 0:\n",
    "        tokenized_sentences.append(ids)\n",
    "\n",
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def5e5b",
   "metadata": {},
   "source": [
    "### Code: ฟังก์ชันสร้าง batch แบบ MLM+NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ce0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters \n",
    "batch_size = 16          # Adjust depends on GPU capacity\n",
    "max_mask = 20            # Number of masked tokens ต่per sequence\n",
    "max_len = 128            # Max sequence length in Transformer\n",
    "n_segments = 2\n",
    "\n",
    "pad_id = word2id[\"[PAD]\"]\n",
    "cls_id = word2id[\"[CLS]\"]\n",
    "sep_id = word2id[\"[SEP]\"]\n",
    "mask_id = word2id[\"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63491bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences, batch_size, max_len, max_mask):\n",
    "    \"\"\"\n",
    "    sentences: list[list[int]] (แต่ละ element คือ sentence token ids)\n",
    "    return:\n",
    "        input_ids: [B, L]\n",
    "        segment_ids: [B, L]\n",
    "        masked_tokens: [B, max_mask]\n",
    "        masked_pos: [B, max_mask]\n",
    "        is_next: [B]\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "\n",
    "    num_sent = len(sentences)\n",
    "\n",
    "    while positive < batch_size // 2 or negative < batch_size // 2:\n",
    "        # random เลือกสองประโยค\n",
    "        idx_a = randint(0, num_sent - 2)\n",
    "        idx_b = randint(0, num_sent - 1)\n",
    "\n",
    "        tokens_a = sentences[idx_a]\n",
    "        # ถ้าเป็น positive pair ให้ B = A+1\n",
    "        if random() < 0.5:\n",
    "            tokens_b = sentences[idx_a + 1]\n",
    "            is_next = True\n",
    "        else:\n",
    "            tokens_b = sentences[idx_b]\n",
    "            is_next = False\n",
    "\n",
    "        # ตัดความยาว A/B ให้รวมกันไม่เกิน max_len-3 (เผื่อ CLS และ SEP สองตัว)\n",
    "        max_total = max_len - 3\n",
    "        if len(tokens_a) + len(tokens_b) > max_total:\n",
    "            # แบ่งสัดส่วนจากความยาวเดิม\n",
    "            len_a = max(1, int(max_total * len(tokens_a) / (len(tokens_a) + len(tokens_b))))\n",
    "            len_b = max_total - len_a\n",
    "            tokens_a = tokens_a[:len_a]\n",
    "            tokens_b = tokens_b[:len_b]\n",
    "\n",
    "        # token embedding: [CLS] A [SEP] B [SEP]\n",
    "        input_ids = [cls_id] + tokens_a + [sep_id] + tokens_b + [sep_id]\n",
    "\n",
    "        # segment embedding\n",
    "        seg_a_len = 1 + len(tokens_a) + 1\n",
    "        seg_b_len = len(input_ids) - seg_a_len\n",
    "        segment_ids = [0] * seg_a_len + [1] * seg_b_len\n",
    "\n",
    "        # ตรวจซ้ำเพื่อ safety ว่าตอนนี้ยาวไม่เกิน max_len\n",
    "        assert len(input_ids) <= max_len\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        # สร้าง masked LM\n",
    "        n_pred = max(1, int(round(len(input_ids) * 0.15)))\n",
    "        n_pred = min(n_pred, max_mask)\n",
    "\n",
    "        cand_pos = [i for i, tok in enumerate(input_ids)\n",
    "                    if tok not in (cls_id, sep_id)]\n",
    "        shuffle(cand_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "\n",
    "        for pos in cand_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "\n",
    "            r = random()\n",
    "            if r < 0.8:\n",
    "                input_ids[pos] = mask_id       # 80% → [MASK]\n",
    "            elif r < 0.9:\n",
    "                rand_tok = randint(0, vocab_size - 1)  # 10% → random token\n",
    "                input_ids[pos] = rand_tok\n",
    "            else:\n",
    "                pass                           # 10% → unchanged\n",
    "\n",
    "        # padding sequence → ยาวเท่ากันเป๊ะ\n",
    "        if len(input_ids) < max_len:\n",
    "            pad_n = max_len - len(input_ids)\n",
    "            input_ids += [pad_id] * pad_n\n",
    "            segment_ids += [0] * pad_n\n",
    "\n",
    "        # padding masked tokens/pos\n",
    "        if len(masked_tokens) < max_mask:\n",
    "            pad_n = max_mask - len(masked_tokens)\n",
    "            masked_tokens += [0] * pad_n\n",
    "            masked_pos += [0] * pad_n\n",
    "\n",
    "        if is_next and positive < batch_size // 2:\n",
    "            batch.append((input_ids, segment_ids, masked_tokens, masked_pos, 1))\n",
    "            positive += 1\n",
    "        elif (not is_next) and negative < batch_size // 2:\n",
    "            batch.append((input_ids, segment_ids, masked_tokens, masked_pos, 0))\n",
    "            negative += 1\n",
    "\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(\n",
    "        torch.LongTensor, zip(*batch)\n",
    "    )\n",
    "    return input_ids, segment_ids, masked_tokens, masked_pos, is_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebdddd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 128]),\n",
       " torch.Size([8, 128]),\n",
       " torch.Size([8, 20]),\n",
       " torch.Size([8, 20]),\n",
       " torch.Size([8]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test building one batch to check shape \n",
    "test_input_ids, test_segment_ids, test_masked_tokens, test_masked_pos, test_is_next = \\\n",
    "    make_batch(tokenized_sentences, batch_size=8, max_len=max_len, max_mask=max_mask)\n",
    "\n",
    "test_input_ids.shape, test_segment_ids.shape, test_masked_tokens.shape, test_masked_pos.shape, test_is_next.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ae4d1",
   "metadata": {},
   "source": [
    "### Code: โมเดล BERT (copy/ปรับจาก BERT.ipynb ให้เล็กลงนิดหน่อย)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d395b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "n_layers = 2    # encoder layers\n",
    "n_heads = 4\n",
    "d_model = 256\n",
    "d_ff = d_model * 4\n",
    "d_k = 64\n",
    "d_v = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e26a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, n_segments):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        bsz, seq_len = x.size()\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)\n",
    "        emb = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78f807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, pad_id=0):\n",
    "    \"\"\"\n",
    "    seq_q: [B, Lq], seq_k: [B, Lk]\n",
    "    return: [B, Lq, Lk]\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.eq(pad_id).unsqueeze(1)  # [B, 1, Lk]\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [B, Lq, Lk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b6aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q: [B, H, Lq, d_k], K: [B, H, Lk, d_k], V: [B, H, Lk, d_v]\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(Q.size(-1))\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc928a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = None\n",
    "        self.sdpa = ScaledDotProductAttention()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        residual = Q\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # [B, L, H*d_k] -> [B, H, L, d_k]\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        # attn_mask: [B, Lq, Lk] -> [B, H, Lq, Lk]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "        context, attn = self.sdpa(q_s, k_s, v_s, attn_mask)\n",
    "        self.attn = attn\n",
    "\n",
    "        # [B, H, L, d_v] -> [B, L, H*d_v]\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.n_heads * self.d_v\n",
    "        )\n",
    "        output = self.fc(context)\n",
    "        return self.layer_norm(output + residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5489d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.layer_norm(x + residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, d_ff, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        out = self.self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        out = self.pos_ffn(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9d082e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_k, d_v, d_ff, n_layers, n_heads, max_len, n_segments):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, max_len, n_segments)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_k, d_v, d_ff, n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # NSP head\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # MLM head\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # decoder share weight with token embedding\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    # NEW: encoder-only method\n",
    "    def encode(self, input_ids, segment_ids):\n",
    "        # Compute encoder hidden states given token ids and segment ids.\n",
    "        # This is used by SentenceEncoder in Task 2.\n",
    "        x = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, pad_id=pad_id)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_self_attn_mask)\n",
    "        return x  # [B, L, d_model]\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        # Use encode() to get contextual token representations\n",
    "        x = self.encode(input_ids, segment_ids)\n",
    "\n",
    "        # NSP head using [CLS] token at position 0\n",
    "        pooled = self.activ(self.fc(x[:, 0]))\n",
    "        logits_nsp = self.classifier(pooled)\n",
    "\n",
    "        # MLM head using masked positions\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        h_masked = torch.gather(x, 1, masked_pos)\n",
    "        h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        x = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, pad_id=pad_id)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_self_attn_mask)\n",
    "\n",
    "        # NSP: ใช้ [CLS] token (ตำแหน่ง 0)\n",
    "        pooled = self.activ(self.fc(x[:, 0]))\n",
    "        logits_nsp = self.classifier(pooled)\n",
    "\n",
    "        # MLM: ดึง hidden states ที่ตำแหน่ง masked_pos\n",
    "        masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        h_masked = torch.gather(x, 1, masked_pos)\n",
    "        h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "        return logits_lm, logits_nsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f978bbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.214033"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_ff=d_ff,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    max_len=max_len,\n",
    "    n_segments=n_segments\n",
    ").to(device)\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6  # ดูประมาณ parameter (ล้าน)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b274c",
   "metadata": {},
   "source": [
    "###  Code: training loop + save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "148491c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3         # This affect amount of time for training\n",
    "lr = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1981924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd04e314b874bfba2b4227655bc9024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss = 6.3761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8d85de5c13496c9e9c80a3d6d7af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss = 3.5728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a860972f4d6943ddb910bd843276d433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss = 2.9685\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_steps = 1000   # limit steps for each epoch to not make the training goes too long\n",
    "\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        input_ids, segment_ids, masked_tokens, masked_pos, is_next = make_batch(\n",
    "            tokenized_sentences,\n",
    "            batch_size=batch_size,\n",
    "            max_len=max_len,\n",
    "            max_mask=max_mask\n",
    "        )\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        masked_tokens = masked_tokens.to(device)\n",
    "        masked_pos = masked_pos.to(device)\n",
    "        is_next = is_next.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "        # MLM loss\n",
    "        # logits_lm: [B, max_mask, vocab_size]\n",
    "        loss_lm = criterion(\n",
    "            logits_lm.view(-1, vocab_size),\n",
    "            masked_tokens.view(-1)\n",
    "        )\n",
    "\n",
    "        # NSP loss\n",
    "        loss_nsp = criterion(logits_nsp, is_next)\n",
    "\n",
    "        loss = loss_lm + loss_nsp\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_steps\n",
    "    print(f\"Epoch {epoch+1:02d} | loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e47d0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/bert_pretrained_from_scratch.pt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights for next Task\n",
    "os.makedirs(\"../model\", exist_ok=True)\n",
    "save_path = \"../model/bert_pretrained_from_scratch.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79882b4",
   "metadata": {},
   "source": [
    "## Sentence Embedding with Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368d2c0",
   "metadata": {},
   "source": [
    "In this section, we reuse the BERT encoder trained in Task 1 as a sentence encoder, \n",
    "build a Siamese architecture like Sentence-BERT, and train it on NLI data \n",
    "(SNLI + MNLI) with the softmax classification objective as described in the assignment [1][2].\n",
    "\n",
    "[1] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers & Gurevych, EMNLP 2019).  \n",
    "[2] SNLI / MNLI datasets from HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "660c73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra libraries for Task 2 (NLI datasets, dataloaders)\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938648",
   "metadata": {},
   "source": [
    "### Load SNLI and MNLI and prepare DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d426a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SNLI and MNLI datasets from HuggingFace\n",
    "# SNLI: natural language inference with three labels (entailment, neutral, contradiction)\n",
    "# MNLI: multi-genre NLI with the same three labels plus some -1 labels that we will filter out\n",
    "\n",
    "snli = load_dataset(\"snli\")\n",
    "mnli = load_dataset(\"glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fc0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid labels (-1) from SNLI (no gold label could be decided)\n",
    "def filter_valid_snli(example):\n",
    "    # Keep only rows where label is 0, 1, or 2\n",
    "    return example[\"label\"] != -1\n",
    "\n",
    "snli = snli.filter(filter_valid_snli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f7a1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each MNLI split and keep only label + text pairs\n",
    "for split_name in list(mnli.keys()):\n",
    "    if \"idx\" in mnli[split_name].column_names:\n",
    "        mnli[split_name] = mnli[split_name].remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e03eb134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a merged DatasetDict: concatenate SNLI and MNLI train/validation/test sets\n",
    "# We shuffle and optionally subsample for faster training.\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "max_train = 10000   # You can increase this if you have more compute\n",
    "max_val = 2000\n",
    "max_test = 2000\n",
    "\n",
    "rawdataset = DatasetDict({\n",
    "    \"train\": concatenate_datasets([\n",
    "        snli[\"train\"],\n",
    "        mnli[\"train\"]\n",
    "    ]).shuffle(seed=55).select(range(max_train)),\n",
    "    \"validation\": concatenate_datasets([\n",
    "        snli[\"validation\"],\n",
    "        mnli[\"validation_matched\"]\n",
    "    ]).shuffle(seed=55).select(range(max_val)),\n",
    "    \"test\": concatenate_datasets([\n",
    "        snli[\"test\"],\n",
    "        mnli[\"test_matched\"]\n",
    "    ]).shuffle(seed=55).select(range(max_test)),\n",
    "})\n",
    "\n",
    "rawdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd4733",
   "metadata": {},
   "source": [
    "### Simple whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0d5e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple whitespace tokenizer reusing the vocabulary from Task 1\n",
    "# We will map unknown words to a special [UNK] id if needed.\n",
    "\n",
    "# Create UNK token if it does not exist\n",
    "if \"[UNK]\" not in word2id:\n",
    "    unk_id = len(word2id)\n",
    "    word2id[\"[UNK]\"] = unk_id\n",
    "    id2word[unk_id] = \"[UNK]\"\n",
    "else:\n",
    "    unk_id = word2id[\"[UNK]\"]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "# Define a helper to tokenize and numericalize a sentence using the existing vocab\n",
    "def encode_sentence(text, max_len):\n",
    "    # Lowercase and basic cleaning to be consistent with Task 1 preprocessing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9 ,.!?'-]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        if w in word2id:\n",
    "            tokens.append(word2id[w])\n",
    "        else:\n",
    "            tokens.append(unk_id)\n",
    "    # Truncate to max_len (we will still add CLS/SEP at sentence encoder level)\n",
    "    if len(tokens) > max_len - 2:  # leave room for CLS/SEP if needed\n",
    "        tokens = tokens[:max_len - 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b8f1d",
   "metadata": {},
   "source": [
    "### Preprocess NLI dataset ids + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c87a24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f032a31eeea4f4da82f244cf419c77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec9d926811c4ea6882e4fa528e45bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ded3744ebb43a388c8451afa26f114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids_a', 'input_ids_b'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids_a', 'input_ids_b'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids_a', 'input_ids_b'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess function to convert premise/hypothesis strings to token id sequences\n",
    "# using the encoder from Task 1. We keep the raw sequences (without CLS/SEP)\n",
    "# because we will let the sentence encoder add those later if needed.\n",
    "\n",
    "def preprocess_nli(example):\n",
    "    premise = example[\"premise\"] if \"premise\" in example else example[\"sentence1\"]\n",
    "    hypothesis = example[\"hypothesis\"] if \"hypothesis\" in example else example[\"sentence2\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    input_ids_a = encode_sentence(premise, max_len)\n",
    "    input_ids_b = encode_sentence(hypothesis, max_len)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids_a\": input_ids_a,\n",
    "        \"input_ids_b\": input_ids_b,\n",
    "        \"label\": label,\n",
    "    }\n",
    "\n",
    "tokenized_nli = rawdataset.map(preprocess_nli, remove_columns=rawdataset[\"train\"].column_names)\n",
    "tokenized_nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e44e0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set PyTorch format for DataLoader and let HuggingFace handle variable-length lists.\n",
    "# tokenized_nli.set_format(\n",
    "#     type=\"torch\",\n",
    "#     columns=[\"input_ids_a\", \"input_ids_b\", \"label\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6039222",
   "metadata": {},
   "source": [
    "### collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "846593e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to pad variable-length sequences in a batch\n",
    "# We will pad both premises and hypotheses to the maximum length in the batch.\n",
    "\n",
    "def collate_nli(batch):\n",
    "    # Extract sequences and labels from the list of examples.\n",
    "    # Each example has Python lists for 'input_ids_a' and 'input_ids_b',\n",
    "    # and an integer label.\n",
    "\n",
    "    # Make sure we are always working on Python lists.\n",
    "    input_ids_a = [list(item[\"input_ids_a\"]) for item in batch]\n",
    "    input_ids_b = [list(item[\"input_ids_b\"]) for item in batch]\n",
    "    labels = torch.tensor([int(item[\"label\"]) for item in batch], dtype=torch.long)\n",
    "\n",
    "    # Compute max lengths for A and B in this batch\n",
    "    max_len_a = max(len(x) for x in input_ids_a)\n",
    "    max_len_b = max(len(x) for x in input_ids_b)\n",
    "\n",
    "    # Pad sequences with [PAD] id\n",
    "    padded_a = []\n",
    "    padded_b = []\n",
    "    for a, b in zip(input_ids_a, input_ids_b):\n",
    "        # Pad premise\n",
    "        pad_len_a = max_len_a - len(a)\n",
    "        padded_a.append(a + [pad_id] * pad_len_a)\n",
    "        # Pad hypothesis\n",
    "        pad_len_b = max_len_b - len(b)\n",
    "        padded_b.append(b + [pad_id] * pad_len_b)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids_a = torch.tensor(padded_a, dtype=torch.long)\n",
    "    input_ids_b = torch.tensor(padded_b, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_a\": input_ids_a,\n",
    "        \"input_ids_b\": input_ids_b,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "981e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataLoaders for train/validation/test\n",
    "batch_size_sbert = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_nli[\"train\"],\n",
    "    batch_size=batch_size_sbert,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_nli,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    tokenized_nli[\"validation\"],\n",
    "    batch_size=batch_size_sbert,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_nli,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    tokenized_nli[\"test\"],\n",
    "    batch_size=batch_size_sbert,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_nli,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb45a2",
   "metadata": {},
   "source": [
    "### Build sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e0752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reuse the BERT encoder implementation from Task 1.\n",
    "# Here we define a helper that takes token ids of a single sentence,\n",
    "# wraps them with [CLS] and [SEP], feeds them into BERT, and returns\n",
    "# the mean-pooled sentence embedding (excluding padding).\n",
    "\n",
    "def build_sentence_batch(input_ids_batch):\n",
    "    # input_ids_batch: [B, L] (token ids without CLS/SEP)\n",
    "    batch_size, seq_len = input_ids_batch.size()\n",
    "    \n",
    "    # Add CLS and SEP to each sentence: [CLS] sentence [SEP]\n",
    "    cls = torch.full((batch_size, 1), cls_id, dtype=torch.long, device=input_ids_batch.device)\n",
    "    sep = torch.full((batch_size, 1), sep_id, dtype=torch.long, device=input_ids_batch.device)\n",
    "\n",
    "    # Concatenate CLS + sentence + SEP along sequence dimension\n",
    "    # Resulting shape: [B, L+2]\n",
    "    input_ids = torch.cat([cls, input_ids_batch, sep], dim=1)\n",
    "\n",
    "    # Build segment ids (all zeros because we only have one sentence)\n",
    "    segment_ids = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "\n",
    "    # Compute positions in the sentence where token is not PAD\n",
    "    # We treat PAD as [PAD] id and do not include it in attention.\n",
    "    return input_ids, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5239e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling over non-PAD token positions. This is similar in spirit\n",
    "# to the pooling used in the SBERT reference code, but operates on our custom BERT.\n",
    "\n",
    "def mean_pool(hidden_states, input_ids):\n",
    "    # hidden_states: [B, L, D]\n",
    "    # input_ids: [B, L] (with PAD tokens)\n",
    "    # Build mask where input_ids != PAD\n",
    "    mask = (input_ids != pad_id).unsqueeze(-1)  # [B, L, 1]\n",
    "    masked_hidden = hidden_states * mask\n",
    "    # Sum and then divide by the number of non-PAD tokens\n",
    "    sum_hidden = masked_hidden.sum(dim=1)                # [B, D]\n",
    "    lengths = mask.sum(dim=1).clamp(min=1)               # [B, 1]\n",
    "    mean_hidden = sum_hidden / lengths                   # [B, D]\n",
    "    return mean_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6171f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper around the BERT model to get sentence embeddings directly.\n",
    "# We reuse the same BERT class and only use the encoder outputs.\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "    def forward(self, input_ids_batch):\n",
    "        # Build full input_ids and segment_ids with CLS/SEP\n",
    "        input_ids, segment_ids = build_sentence_batch(input_ids_batch)\n",
    "        \n",
    "        # Pass through the BERT encoder to obtain hidden states\n",
    "        logits_lm, logits_nsp = self.bert(input_ids, segment_ids, masked_pos=torch.zeros(input_ids.size(0), 1, dtype=torch.long, device=input_ids.device))\n",
    "        \n",
    "        # The BERT forward returns MLM and NSP logits, not hidden states.\n",
    "        # To get hidden states, we need to slightly modify BERT or define a method that exposes them.\n",
    "        # For simplicity, we will define a new function in BERT that returns hidden states.\n",
    "\n",
    "        raise NotImplementedError(\"Please modify BERT class to expose hidden states.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce566",
   "metadata": {},
   "source": [
    "### Modify BERT class to return hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd1547c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this method inside the BERT class definition from Task 1\n",
    "\n",
    "def encode(self, input_ids, segment_ids):\n",
    "    # Compute encoder hidden states given token ids and segment ids.\n",
    "    # This method does not compute MLM or NSP logits; it only returns the final hidden states.\n",
    "    x = self.embedding(input_ids, segment_ids)\n",
    "    enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, pad_id=pad_id)\n",
    "    for layer in self.layers:\n",
    "        x = layer(x, enc_self_attn_mask)\n",
    "    return x  # [B, L, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b9e5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_ids, segment_ids, masked_pos):\n",
    "    # Encode input sequence to obtain contextual token representations\n",
    "    x = self.encode(input_ids, segment_ids)\n",
    "\n",
    "    # NSP head using [CLS] token at position 0\n",
    "    pooled = self.activ(self.fc(x[:, 0]))\n",
    "    logits_nsp = self.classifier(pooled)\n",
    "\n",
    "    # MLM head using masked positions\n",
    "    masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "    h_masked = torch.gather(x, 1, masked_pos)\n",
    "    h_masked = self.norm(F.gelu(self.linear(h_masked)))\n",
    "    logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "    return logits_lm, logits_nsp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352165cc",
   "metadata": {},
   "source": [
    "### SentenceEncoder encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3088357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceEncoder now uses the encode() method to get hidden states\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "    def forward(self, input_ids_batch):\n",
    "        # Build full sentence batch with CLS and SEP tokens\n",
    "        input_ids, segment_ids = build_sentence_batch(input_ids_batch)\n",
    "        input_ids = input_ids.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        # Use BERT encoder to get contextualized token representations\n",
    "        hidden_states = self.bert.encode(input_ids, segment_ids)  # [B, L, D]\n",
    "\n",
    "        # Compute mean-pooled sentence embedding\n",
    "        sentence_embeddings = mean_pool(hidden_states, input_ids)  # [B, D]\n",
    "\n",
    "        return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "473641ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh BERT model with the same architecture as in Task 1\n",
    "bert_for_sbert = BERT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    d_k=d_k,\n",
    "    d_v=d_v,\n",
    "    d_ff=d_ff,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    max_len=max_len,\n",
    "    n_segments=n_segments\n",
    ")\n",
    "\n",
    "# Load pretrained weights from Task 1\n",
    "task1_ckpt_path = \"../model/bert_pretrained_from_scratch.pt\"\n",
    "state_dict = torch.load(task1_ckpt_path, map_location=\"cpu\")\n",
    "bert_for_sbert.load_state_dict(state_dict)\n",
    "\n",
    "bert_for_sbert = bert_for_sbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fc9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentence encoder wrapper\n",
    "sentence_encoder = SentenceEncoder(bert_for_sbert).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2adb333",
   "metadata": {},
   "source": [
    "### Softmax classification head (u, v, |u−v| concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8d35d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the assignment and the SBERT paper, we build a classifier\n",
    "# on top of the concatenation [u; v; |u - v|], where u and v are the sentence embeddings.\n",
    "# If the embedding dimension is d_model, the input to the classifier has size 3 * d_model.\n",
    "\n",
    "class SBERTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(3 * hidden_dim, 3)  # three NLI labels\n",
    "\n",
    "    def forward(self, input_ids_a, input_ids_b):\n",
    "        # Compute sentence embeddings u and v for premise and hypothesis\n",
    "        u = self.encoder(input_ids_a)  # [B, D]\n",
    "        v = self.encoder(input_ids_b)  # [B, D]\n",
    "\n",
    "        # Compute absolute difference |u - v|\n",
    "        uv_abs = torch.abs(u - v)\n",
    "\n",
    "        # Concatenate u, v, and |u - v| along the last dimension\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)  # [B, 3D]\n",
    "\n",
    "        # Compute logits for three classes (entailment, neutral, contradiction)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51253e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SBERT classifier with the sentence encoder and dimension d_model\n",
    "sbert_model = SBERTClassifier(sentence_encoder, hidden_dim=d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc319b",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "432c3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function for NLI classification\n",
    "# We use cross-entropy loss as in the SBERT classification objective.\n",
    "\n",
    "lr_sbert = 2e-5\n",
    "num_epochs_sbert = 3  # You can change this depending on time and resources\n",
    "\n",
    "criterion_nli = nn.CrossEntropyLoss()\n",
    "optimizer_sbert = optim.Adam(sbert_model.parameters(), lr=lr_sbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8650891a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d45c9625094931bb7b42c48c2741c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SBERT Training Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | SBERT training loss = 1.1026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6a1d8b7d28443eb4b6df6930ba0522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SBERT Training Epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | SBERT training loss = 1.0822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ea49706d44f41badefd098f22e9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SBERT Training Epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | SBERT training loss = 1.0711\n"
     ]
    }
   ],
   "source": [
    "# Training loop for Sentence-BERT classifier on NLI data\n",
    "# For each batch, we compute sentence embeddings for premise and hypothesis,\n",
    "# build the [u; v; |u - v|] representation, and optimize the softmax loss.\n",
    "\n",
    "for epoch in range(num_epochs_sbert):\n",
    "    sbert_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"SBERT Training Epoch {epoch+1}\"):\n",
    "        # Move all batch tensors to the active device\n",
    "        input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "        input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Reset gradients at the start of each step\n",
    "        optimizer_sbert.zero_grad()\n",
    "\n",
    "        # Forward pass: compute logits for NLI labels\n",
    "        logits = sbert_model(input_ids_a, input_ids_b)\n",
    "\n",
    "        # Compute cross-entropy loss between predicted logits and true labels\n",
    "        loss = criterion_nli(logits, labels)\n",
    "\n",
    "        # Backpropagate and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer_sbert.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_steps += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_steps)\n",
    "    print(f\"Epoch {epoch+1:02d} | SBERT training loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8295d3a",
   "metadata": {},
   "source": [
    "### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7923a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after SBERT training: 0.4125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SBERT classifier on the validation set to monitor performance.\n",
    "# We compute simple accuracy over the three NLI labels.\n",
    "\n",
    "def evaluate_sbert(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids_a, input_ids_b)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / max(1, total)\n",
    "    return accuracy\n",
    "\n",
    "val_acc = evaluate_sbert(sbert_model, val_loader)\n",
    "print(f\"Validation accuracy after SBERT training: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b05b2",
   "metadata": {},
   "source": [
    "### Save sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e38cfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model/sbert_sentence_encoder.pt'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained SBERT sentence encoder weights for later use (Task 3 and Task 4).\n",
    "# We save the full SBERT classifier, but you can also save only the encoder if preferred.\n",
    "\n",
    "os.makedirs(\"../model\", exist_ok=True)\n",
    "sbert_ckpt_path = \"../model/sbert_sentence_encoder.pt\"\n",
    "torch.save(sbert_model.state_dict(), sbert_ckpt_path)\n",
    "sbert_ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697a169",
   "metadata": {},
   "source": [
    "## Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247df024",
   "metadata": {},
   "source": [
    "In this section, we evaluate the Sentence-BERT model from Task 2 on the NLI task \n",
    "and report a classification report (precision, recall, F1, support) on a held-out \n",
    "test set. We then discuss limitations and potential improvements [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b31579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in test set: [-1  0  1  2]\n"
     ]
    }
   ],
   "source": [
    "# Quick check of unique labels in the collected test set\n",
    "sbert_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        labels = batch[\"labels\"]\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "import numpy as np\n",
    "print(\"Unique labels in test set:\", np.unique(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72d32ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn metrics that we will use for the detailed classification report.\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the SBERT classifier on the test set and collect all predictions and labels.\n",
    "# We then compute a classification report only over the valid NLI labels (0, 1, 2).\n",
    "def get_classification_report(model, data_loader):\n",
    "    # Switch the model to evaluation mode to disable dropout and gradient computation.\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient tracking since we are only doing inference.\n",
    "    with torch.no_grad():\n",
    "        # Iterate over all batches in the test DataLoader.\n",
    "        for batch in data_loader:\n",
    "            # Move the batch tensors to the active device (CPU/GPU).\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass through the SBERT classifier to obtain logits.\n",
    "            logits = model(input_ids_a, input_ids_b)\n",
    "\n",
    "            # Take the argmax over the class dimension to get predicted labels.\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "            # Store predictions and true labels on CPU for later metrics calculation.\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Convert to numpy arrays for easier masking and inspection.\n",
    "    all_preds_arr = np.array(all_preds)\n",
    "    all_labels_arr = np.array(all_labels)\n",
    "\n",
    "    # Keep only examples with valid labels {0, 1, 2} to match the three NLI classes.\n",
    "    valid_mask = np.isin(all_labels_arr, [0, 1, 2])\n",
    "    all_preds_arr = all_preds_arr[valid_mask]\n",
    "    all_labels_arr = all_labels_arr[valid_mask]\n",
    "\n",
    "    # Define human-readable names for the three NLI classes.\n",
    "    target_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    labels = [0, 1, 2]\n",
    "\n",
    "    # Compute the classification report string using sklearn, specifying labels explicitly.\n",
    "    report_str = classification_report(\n",
    "        all_labels_arr,\n",
    "        all_preds_arr,\n",
    "        labels=labels,\n",
    "        target_names=target_names,\n",
    "        digits=2\n",
    "    )\n",
    "    return report_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a508c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.44      0.79      0.57       365\n",
      "      neutral       0.46      0.39      0.42       309\n",
      "contradiction       0.42      0.11      0.17       329\n",
      "\n",
      "     accuracy                           0.44      1003\n",
      "    macro avg       0.44      0.43      0.39      1003\n",
      " weighted avg       0.44      0.44      0.39      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_report = get_classification_report(sbert_model, test_loader)\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2458",
   "metadata": {},
   "source": [
    "### Classification report on NLI test set\n",
    "\n",
    "```text\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "   entailment       0.44      0.79      0.57       365\n",
    "      neutral       0.46      0.39      0.42       309\n",
    "contradiction       0.42      0.11      0.17       329\n",
    "\n",
    "     accuracy                           0.44      1003\n",
    "    macro avg       0.44      0.43      0.39      1003\n",
    " weighted avg       0.44      0.44      0.39      1003\n",
    "```\n",
    "\n",
    "- **Entailment** is detected with relatively high recall (0.79), meaning the model correctly finds most entailment pairs, but the precision is moderate (0.44), so there are many false positives.  \n",
    "- **Neutral** has balanced precision and recall around 0.4, indicating that the model struggles to separate neutral from the other two classes.  \n",
    "- **Contradiction** shows low recall (0.11), meaning the model misses most contradiction cases even though the precision is similar to the other classes, suggesting the classifier rarely predicts this label.\n",
    "\n",
    "These results are reasonable for a small custom BERT (2 layers, 256 hidden size) pretrained on only ~100k sentences and fine-tuned on a reduced subset of SNLI+MNLI rather than the full datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8e75e5",
   "metadata": {},
   "source": [
    "## Datasets and preprocessing\n",
    "\n",
    "- Pretraining corpus (Task 1):  \n",
    "  We sampled 100k sentences from the BookCorpus dataset via the HuggingFace Hub \n",
    "  (`bookcorpus`, `plain_text`, `trust_remote_code=True`). Sentences were lowercased, \n",
    "  cleaned with a simple regex, and tokenized by whitespace into a custom word-level \n",
    "  vocabulary [1][3].\n",
    "\n",
    "- NLI datasets (Task 2 and 3):  \n",
    "  We combined the SNLI dataset and the MNLI dataset from the GLUE benchmark using \n",
    "  the HuggingFace `datasets` library. For SNLI, examples with label `-1` were \n",
    "  removed because no gold label was available. For MNLI, the `idx` column was \n",
    "  removed and only the text and label fields were kept [1][3].\n",
    "\n",
    "- Train/validation/test splits:  \n",
    "  From the concatenated SNLI + MNLI DatasetDict, we sampled a subset for training, \n",
    "  validation, and testing (e.g., 10k train, 2k validation, 2k test) using \n",
    "  `shuffle(seed=55).select(range(N))` for efficiency in this assignment setting [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ef120",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BERT pretraining (Task 1):  \n",
    "  - Number of encoder layers: 2  \n",
    "  - Hidden size (`d_model`): 256  \n",
    "  - Number of attention heads: 4  \n",
    "  - Feed-forward size (`d_ff`): 4 × 256  \n",
    "  - Maximum sequence length: 128  \n",
    "  - Batch size: (e.g., 16)  \n",
    "  - Learning rate: 1e-4 using Adam  \n",
    "  - Pretraining steps per epoch: 1000 batches of randomly sampled sentence pairs  \n",
    "  - Pretraining objective: combined masked language modeling (MLM) and \n",
    "    next sentence prediction (NSP).\n",
    "\n",
    "- Sentence-BERT fine-tuning (Task 2):  \n",
    "  - Base encoder: BERT model from Task 1 (weights loaded from \n",
    "    `bert_pretrained_from_scratch.pt`)  \n",
    "  - Batch size: 32  \n",
    "  - Optimizer: Adam with learning rate 2e-5  \n",
    "  - Number of epochs: 3  \n",
    "  - Sentence pooling: mean pooling over non-PAD tokens after the encoder  \n",
    "  - Classification head: single linear layer on top of the concatenation \n",
    "    `[u; v; |u − v|]` with output dimension 3 (entailment, neutral, contradiction) [1][3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddb699",
   "metadata": {},
   "source": [
    "## Modifications to the original models\n",
    "\n",
    "- BERT architecture:  \n",
    "  The original classroom BERT implementation (from `BERT.ipynb`) was adapted to a \n",
    "  smaller configuration (2 layers, 256-dimensional hidden states, 4 attention heads) \n",
    "  to reduce computational cost while preserving the key architectural components \n",
    "  (token/position/segment embeddings, multi-head self-attention, feed-forward layers, \n",
    "  masked LM head, NSP head) [2][5].\n",
    "\n",
    "- Additional encoder interface:  \n",
    "  We added an `encode(input_ids, segment_ids)` method to the BERT class that returns \n",
    "  the final encoder hidden states without computing MLM or NSP logits, enabling reuse \n",
    "  of the encoder as a sentence encoder in Sentence-BERT [2][5].\n",
    "\n",
    "- Sentence-BERT wrapper:  \n",
    "  We wrapped the BERT encoder in a `SentenceEncoder` module that adds `[CLS]` and \n",
    "  `[SEP]` tokens, runs the encoder, and applies mean pooling over non-PAD positions \n",
    "  to produce fixed-size sentence embeddings, following the design of Sentence-BERT [3][5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabff5a",
   "metadata": {},
   "source": [
    "## Limitations and potential improvements\n",
    "\n",
    "- Limited pretraining data and capacity:  \n",
    "  The BERT model was pretrained on only a 100k-sentence subset of BookCorpus with \n",
    "  a small 2-layer encoder, which is significantly smaller than standard BERT-base \n",
    "  pretraining (800M+ words). This limits the quality of the learned representations \n",
    "  and likely caps the final NLI accuracy [1][5].\n",
    "\n",
    "- Simple tokenization:  \n",
    "  We used a custom whitespace-based word-level tokenizer rather than a subword \n",
    "  tokenizer (WordPiece/BPE), which leads to a large vocabulary, no sharing of rare \n",
    "  subword units, and more out-of-vocabulary issues compared to standard BERT tokenizers [4][5].\n",
    "\n",
    "- Training objective and data size:  \n",
    "  For NLI, we trained only the simple `[u; v; |u − v|]` classifier with cross-entropy \n",
    "  loss on a relatively small subset of SNLI and MNLI. Scaling up the number of training \n",
    "  examples, adding learning-rate schedules or regularization, and exploring alternative \n",
    "  objectives (e.g., contrastive losses from SimCSE) could further improve sentence \n",
    "  embeddings and NLI performance [1][3][5].\n",
    "\n",
    "- Future improvements:  \n",
    "  Potential extensions include using a subword tokenizer from HuggingFace, increasing \n",
    "  the depth and width of the encoder, using the full SNLI/MNLI datasets, and adding \n",
    "  validation-based early stopping or more advanced optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a362b8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
